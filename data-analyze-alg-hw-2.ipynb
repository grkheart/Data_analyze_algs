{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-07-09T12:18:25.396671Z","iopub.execute_input":"2022-07-09T12:18:25.397085Z","iopub.status.idle":"2022-07-09T12:18:25.404703Z","shell.execute_reply.started":"2022-07-09T12:18:25.397049Z","shell.execute_reply":"2022-07-09T12:18:25.403520Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Задача №1","metadata":{}},{"cell_type":"markdown","source":"#### Напишите функцию наподобие gradient_descent_reg_l2, но для применения L1-регуляризации.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2022-07-09T12:18:25.417529Z","iopub.execute_input":"2022-07-09T12:18:25.418332Z","iopub.status.idle":"2022-07-09T12:18:25.423460Z","shell.execute_reply.started":"2022-07-09T12:18:25.418277Z","shell.execute_reply":"2022-07-09T12:18:25.422528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = np.array([[ 1,  1],\n              [ 1,  1],\n              [ 1,  2],\n              [ 1,  5],\n              [ 1,  3],\n              [ 1,  0],\n              [ 1,  5],\n              [ 1, 10],\n              [ 1,  1],\n              [ 1,  2]])\ny = [45, 55, 50, 55, 60, 35, 75, 80, 50, 60]","metadata":{"execution":{"iopub.status.busy":"2022-07-09T12:18:25.438533Z","iopub.execute_input":"2022-07-09T12:18:25.438999Z","iopub.status.idle":"2022-07-09T12:18:25.445611Z","shell.execute_reply.started":"2022-07-09T12:18:25.438959Z","shell.execute_reply":"2022-07-09T12:18:25.444468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.logspace(-3, 2, 50)","metadata":{"execution":{"iopub.status.busy":"2022-07-09T12:18:25.462665Z","iopub.execute_input":"2022-07-09T12:18:25.463379Z","iopub.status.idle":"2022-07-09T12:18:25.471624Z","shell.execute_reply.started":"2022-07-09T12:18:25.463251Z","shell.execute_reply":"2022-07-09T12:18:25.470230Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import Lasso\n\nn = 50\n\ncoeffs = np.zeros((n, train_X.shape[1]))\nalpha_list = np.logspace(-3, 2, n)\n\nfor i, val in enumerate(alpha_list):\n    lasso = Lasso(alpha=val, fit_intercept=False)\n    lasso.fit(train_X, train_y)\n    coeffs[i, :] = lasso.coef_.flatten()\n\nfor i in range(train_X.shape[1]):\n    plt.plot(alpha_list, coeffs[:, i])\n\n    \nplt.title('Убывание абсолютных значений весов признаков\\n при увеличении коэффициента регуляризации alpha (Lasso)')\nplt.xticks(np.arange(0, 101, 10))\nplt.xlabel('alpha')\nplt.ylabel('Вес признака');","metadata":{"execution":{"iopub.status.busy":"2022-07-09T12:18:25.483215Z","iopub.execute_input":"2022-07-09T12:18:25.483866Z","iopub.status.idle":"2022-07-09T12:18:25.721987Z","shell.execute_reply.started":"2022-07-09T12:18:25.483826Z","shell.execute_reply":"2022-07-09T12:18:25.720638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"index = 36\n\ncoef = coeffs[index]\n\nprint(f'Коэффициент регуляризации: {alpha_list[index]}')\nprint(f'Веса: {coef}')\n\nfig, axs = plt.subplots(1, 2, figsize=(15, 4))\nax1, ax2 = axs\n\nax1.scatter(train_X[:, 1], train_y)\n\ny_pred = np.dot(train_X, coef)\nax1.plot(train_X[:, 1], y_pred, c='r')\n\nax1.set_xlabel('X1')\nax1.set_ylabel('Y')\nprint(f'MSE на обучении: {np.mean((y_pred - train_y)**2)}')\n\nax2.scatter(test_X[:, 1], test_y)\n\ny_pred = np.dot(test_X, coef)\nax2.plot(test_X[:, 1], y_pred, c='r')\n\nax2.set_xlabel('X1')\nax2.set_ylabel('Y')\nprint(f'MSE на тесте: {np.mean((y_pred - test_y)**2)}')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-09T12:18:25.723808Z","iopub.execute_input":"2022-07-09T12:18:25.724132Z","iopub.status.idle":"2022-07-09T12:18:26.024069Z","shell.execute_reply.started":"2022-07-09T12:18:25.724102Z","shell.execute_reply":"2022-07-09T12:18:26.022868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def gradient_descent_reg_l1from sklearn.datasets import make_regression\nimport matplotlib\nfrom matplotlib import pyplot as plt\nimport numpy as np(X, y, iterations, eta=1e-4, reg=1e-8):\n    W = np.random.randn(X.shape[1])\n    n = X.shape[0]\n    \n    for i in range(0, iterations):\n        y_pred = np.dot(X, W)\n        err = calc_mse(y, y_pred)\n        \n        dQ = 2/n * X.T @ (y_pred - y) # градиент функции ошибки\n        dReg = reg * W # градиент регуляризации\n        \n        W -= eta * (dQ + dReg)\n        \n        if i % (iterations / 10) == 0:\n            print(f'Iter: {i}, weights: {W}, error {err}')\n    \n    print(f'Final MSE: {calc_mse(y, np.dot(X, W))}')\n    return W","metadata":{"execution":{"iopub.status.busy":"2022-07-09T12:18:26.025656Z","iopub.execute_input":"2022-07-09T12:18:26.026549Z","iopub.status.idle":"2022-07-09T12:18:26.035870Z","shell.execute_reply.started":"2022-07-09T12:18:26.026501Z","shell.execute_reply":"2022-07-09T12:18:26.034682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gradient_descent_reg_l1(X_st, y, iterations=5000, eta=1e-1, reg=1e-4)","metadata":{"execution":{"iopub.status.busy":"2022-07-09T12:18:26.038980Z","iopub.execute_input":"2022-07-09T12:18:26.039455Z","iopub.status.idle":"2022-07-09T12:18:26.062708Z","shell.execute_reply.started":"2022-07-09T12:18:26.039411Z","shell.execute_reply":"2022-07-09T12:18:26.061258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"НЕ ПОНЯЛ ЗАДАНИЕ КАК НАПИСАТЬ L1 на основе L2","metadata":{}},{"cell_type":"markdown","source":"### Задание №2\n\n*Можно ли к одному и тому же признаку применить сразу и нормализацию, и стандартизацию?","metadata":{}},{"cell_type":"markdown","source":"Если я правильно понял то нормализацию и стандартизацию можно использовать вместе. Если того требует задача например, чтобы все данные были в диапазоне от 0 до 1 или от –1 до 1.","metadata":{}},{"cell_type":"markdown","source":"### Задание №3\n\n\n*Сгенерируйте датасет при помощи sklearn.datasets.make_regression и обучите линейную модель при помощи градиентного и стохастического градиентного спуска. Нанесите среднеквадратичную ошибку для обоих методов на один график, сделайте выводы о разнице скорости сходимости каждого из методов.","metadata":{}},{"cell_type":"code","source":"from sklearn.datasets import make_regression\nimport matplotlib\nfrom matplotlib import pyplot as plt\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2022-07-09T12:51:17.353674Z","iopub.execute_input":"2022-07-09T12:51:17.354134Z","iopub.status.idle":"2022-07-09T12:51:17.432167Z","shell.execute_reply.started":"2022-07-09T12:51:17.354098Z","shell.execute_reply":"2022-07-09T12:51:17.431141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_fetures = 2       # число признаков - 2\nn_samples = 5000    # количество примеров - 5000","metadata":{"execution":{"iopub.status.busy":"2022-07-09T12:51:31.536688Z","iopub.execute_input":"2022-07-09T12:51:31.537377Z","iopub.status.idle":"2022-07-09T12:51:31.541848Z","shell.execute_reply.started":"2022-07-09T12:51:31.537335Z","shell.execute_reply":"2022-07-09T12:51:31.540636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# генерируем набор данных\ndatar,target, coef = make_regression(n_samples=n_samples, n_features=n_fetures,noise=5, coef=True, random_state=5)","metadata":{"execution":{"iopub.status.busy":"2022-07-09T12:51:45.752016Z","iopub.execute_input":"2022-07-09T12:51:45.752424Z","iopub.status.idle":"2022-07-09T12:51:45.768050Z","shell.execute_reply.started":"2022-07-09T12:51:45.752391Z","shell.execute_reply":"2022-07-09T12:51:45.760952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(datar[:,0],target,'.')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-09T12:51:56.774043Z","iopub.execute_input":"2022-07-09T12:51:56.774454Z","iopub.status.idle":"2022-07-09T12:51:57.123178Z","shell.execute_reply.started":"2022-07-09T12:51:56.774420Z","shell.execute_reply":"2022-07-09T12:51:57.121903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"datar.shape","metadata":{"execution":{"iopub.status.busy":"2022-07-09T12:52:09.980644Z","iopub.execute_input":"2022-07-09T12:52:09.982008Z","iopub.status.idle":"2022-07-09T12:52:09.989737Z","shell.execute_reply.started":"2022-07-09T12:52:09.981949Z","shell.execute_reply":"2022-07-09T12:52:09.988393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"coef","metadata":{"execution":{"iopub.status.busy":"2022-07-09T12:52:24.249400Z","iopub.execute_input":"2022-07-09T12:52:24.249848Z","iopub.status.idle":"2022-07-09T12:52:24.258345Z","shell.execute_reply.started":"2022-07-09T12:52:24.249804Z","shell.execute_reply":"2022-07-09T12:52:24.256756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def mserror(X, w, y_pred):\n    \"\"\"\n    функция для  вычисления среднеквадратичной ошибки\n    \"\"\"\n    y = X.dot(w)\n    return (sum((y - y_pred)**2)) / len(y)","metadata":{"execution":{"iopub.status.busy":"2022-07-09T12:52:42.876940Z","iopub.execute_input":"2022-07-09T12:52:42.877349Z","iopub.status.idle":"2022-07-09T12:52:42.884668Z","shell.execute_reply.started":"2022-07-09T12:52:42.877313Z","shell.execute_reply":"2022-07-09T12:52:42.883250Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Градиентный спуск**","metadata":{}},{"cell_type":"code","source":"alpha = 0.1                # шаг обучения\nW = np.zeros(n_fetures)    # начальный вектор весов\nerrors = []                # список значений ошибки после каждой итерации\nW_list = [W.copy()]        # список векторов весов после каждой итерации\nmax_iter = 1e5             # максимальное количество итераций\nmin_weight_dist = 1e-8     # критерий сходимости\nweight_dist = np.inf       # начальная разница весов\niter_num = 0               # счетчик\nwhile weight_dist > min_weight_dist and iter_num < max_iter:\n    W_pred = W - 2 * alpha * np.dot(datar.T, (np.dot(datar, W) - target)) / target.shape[0]\n    weight_dist = np.linalg.norm(W_pred - W, ord=2)\n    errors.append(mserror(datar, W_pred, target))\n    W_list.append(W_pred.copy())\n    iter_num += 1\n    W = W_pred\n#     alpha *= 0.99\nW_list = np.array(W_list)\n\nprint(f'В случае использования градиентного спуска функционал ошибки составляет {round(errors[-1], 4)}')","metadata":{"execution":{"iopub.status.busy":"2022-07-09T12:52:59.999366Z","iopub.execute_input":"2022-07-09T12:52:59.999834Z","iopub.status.idle":"2022-07-09T12:53:00.201410Z","shell.execute_reply.started":"2022-07-09T12:52:59.999795Z","shell.execute_reply":"2022-07-09T12:53:00.200114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**SGD**","metadata":{}},{"cell_type":"code","source":"alpha = 0.001\nw = np.zeros(n_fetures)\nerr = []\nw_list = [w.copy()]\nmax_iter = 1e4\nmin_weight_dist = 1e-8\nweight_dist = np.inf\niter_num = 0\nlamb = 0.01                      # коэфициент памяти\nnp.random.seed(1234)\nQ = [mserror(datar, w, target)]  # список потерь\n\nwhile weight_dist > min_weight_dist and iter_num < max_iter:\n    train_ind = np.random.randint(datar.shape[0])\n    \n    new_w = w - 2 * alpha/(0+1) * np.dot(datar[train_ind].T, (np.dot(datar[train_ind], w) - target[train_ind]))\n \n    weight_dist = np.linalg.norm(new_w - w, ord=2)\n    \n    w_list.append(new_w.copy())\n    err.append(mserror(datar, new_w, target))\n    Q.append(Q[-1]*(1-lamb)+lamb*(np.dot(datar[train_ind], w) - target[train_ind])**2)\n    iter_num += 1\n    w = new_w\n#     alpha *= 0.99\n    \nw_list = np.array(w_list)\n \nprint(f'В случае использования стохастического градиентного спуска функционал ошибки составляет '\n      f'{round(err[-1], 4)}')","metadata":{"execution":{"iopub.status.busy":"2022-07-09T12:54:11.161957Z","iopub.execute_input":"2022-07-09T12:54:11.162394Z","iopub.status.idle":"2022-07-09T12:54:30.883796Z","shell.execute_reply.started":"2022-07-09T12:54:11.162358Z","shell.execute_reply":"2022-07-09T12:54:30.882544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Визуализируем**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(13, 6))\nplt.title('MSE')\nplt.xlabel('Iteration number')\nplt.ylabel('MSE')\n\nplt.plot(range(len(errors)), np.log(errors), label='gd')\nplt.plot(range(len(Q)), np.log(Q), label='sgd')\nplt.legend()\nplt.grid(True)\n\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-07-09T12:55:02.926936Z","iopub.execute_input":"2022-07-09T12:55:02.927363Z","iopub.status.idle":"2022-07-09T12:55:03.123083Z","shell.execute_reply.started":"2022-07-09T12:55:02.927328Z","shell.execute_reply":"2022-07-09T12:55:03.121837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Градиентный спуск позволяет производить обучение за меньшее число итераций, но требует большой объем вычислений на каждой итерации, поэтому в случае большого количества данных стохастический градиентный спуск является более предпочтительным. Также для повышения скорости сходимости возможно испольхование методов регуляризации.","metadata":{}}]}